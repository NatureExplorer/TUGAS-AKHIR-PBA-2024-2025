# -*- coding: utf-8 -*-
"""Fast Text.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10oit4A7MdKucZ5_3FubgpY5MrQAHQCcu
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Preprocessing Function
def preprocess(text):
    stop_words = set(stopwords.words('english'))
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text)  # Keep only letters
    words = text.split()
    words = [word for word in words if word not in stop_words]
    return ' '.join(words)

# Define Dataset Class
class AGNewsDataset(Dataset):
    def __init__(self, data, labels, word_to_idx, max_len=100):
        self.data = data
        self.labels = labels
        self.word_to_idx = word_to_idx
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        text = self.data[idx]
        label = self.labels[idx]
        encoded_text = self.encode_text(text)
        return torch.tensor(encoded_text, dtype=torch.long), torch.tensor(label, dtype=torch.long)

    def encode_text(self, text):
        words = text.split()[:self.max_len]
        encoded = [self.word_to_idx.get(word, self.word_to_idx["<UNK>"]) for word in words]
        padded = encoded + [self.word_to_idx["<PAD>"]] * (self.max_len - len(encoded))
        return padded

# Load GloVe Embeddings
def load_glove_embeddings(glove_path, embedding_dim):
    word_to_idx = {"<PAD>": 0, "<UNK>": 1}
    embeddings = [np.zeros(embedding_dim), np.random.uniform(-0.25, 0.25, embedding_dim)]

    with open(glove_path, 'r', encoding='utf-8') as f:
        for idx, line in enumerate(f):
            parts = line.split()
            word = parts[0]
            vector = np.array(parts[1:], dtype=np.float32)
            word_to_idx[word] = len(word_to_idx)
            embeddings.append(vector)

    return word_to_idx, np.array(embeddings)

# FastText Model Definition
class FastText(nn.Module):
    def __init__(self, vocab_size, embedding_dim, num_classes, pretrained_embeddings):
        super(FastText, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.embedding.weight.data.copy_(torch.tensor(pretrained_embeddings, dtype=torch.float32))
        self.embedding.weight.requires_grad = True  # Allow fine-tuning
        self.fc = nn.Sequential(
            nn.Linear(embedding_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        x = self.embedding(x).mean(dim=1)
        return self.fc(x)

# Training Function
def train_model(model, dataloader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    for texts, labels in tqdm(dataloader, desc="Training"):
        texts, labels = texts.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(texts)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(dataloader)

# Evaluation Function
def evaluate_model(model, dataloader, device):
    model.eval()
    all_preds, all_labels = [], []
    with torch.no_grad():
        for texts, labels in dataloader:
            texts, labels = texts.to(device), labels.to(device)
            outputs = model(texts)
            preds = torch.argmax(outputs, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    return classification_report(all_labels, all_preds)

import os
import tarfile
import re
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

# Main Script
if __name__ == "__main__":
    # Paths and Parameters
    data_path = "ag_news_csv.tar.gz"  # Compressed file
    csv_file_name = "ag_news_csv/train.csv"  # Target file inside archive
    glove_path = "/content/glove.6B.100d.txt"  # Path to GloVe file
    embedding_dim = 100
    max_len = 100
    batch_size = 128
    num_classes = 4
    num_epochs = 10
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load Data from Archive
    with tarfile.open(data_path, "r:gz") as tar:
        csv_file = tar.extractfile(csv_file_name)
        if csv_file:
            df = pd.read_csv(csv_file, header=None)
        else:
            raise FileNotFoundError(f"File {csv_file_name} not found in archive {data_path}")

    # Preprocess Data
    df[1] = df[1].apply(preprocess)
    texts, labels = df[1], df[0] - 1

    # Split Data
    train_texts, val_texts, train_labels, val_labels = train_test_split(
        texts, labels, test_size=0.2, random_state=42
    )
    train_texts = train_texts.reset_index(drop=True)
    train_labels = train_labels.reset_index(drop=True)
    val_texts = val_texts.reset_index(drop=True)
    val_labels = val_labels.reset_index(drop=True)

# Load GloVe
word_to_idx, embeddings = load_glove_embeddings(glove_path, embedding_dim)
print(f"Loaded {len(word_to_idx)} words from GloVe.")

# Prepare Datasets and DataLoaders
train_dataset = AGNewsDataset(train_texts.tolist(), train_labels.tolist(), word_to_idx, max_len)
val_dataset = AGNewsDataset(val_texts.tolist(), val_labels.tolist(), word_to_idx, max_len)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# Initialize Model
model = FastText(len(word_to_idx), embedding_dim, num_classes, embeddings).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0005)

# Modified Training Function with Accuracy
def train_model(model, dataloader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    correct_predictions = 0
    total_samples = 0
    for texts, labels in tqdm(dataloader, desc="Training"):
        texts, labels = texts.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(texts)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # Calculate accuracy
        _, predicted = torch.max(outputs, 1)
        correct_predictions += (predicted == labels).sum().item()
        total_samples += labels.size(0)

        total_loss += loss.item()

    accuracy = correct_predictions / total_samples
    return total_loss / len(dataloader), accuracy

# Modified Evaluation Function with Accuracy
def evaluate_model(model, dataloader, device):
    model.eval()
    correct_predictions = 0
    total_samples = 0
    with torch.no_grad():
        for texts, labels in dataloader:
            texts, labels = texts.to(device), labels.to(device)
            outputs = model(texts)
            preds = torch.argmax(outputs, dim=1)

            # Calculate accuracy
            correct_predictions += (preds == labels).sum().item()
            total_samples += labels.size(0)

    accuracy = correct_predictions / total_samples
    return accuracy

# Training and Evaluation Loop with Accuracy
for epoch in range(num_epochs):
    print(f"Epoch {epoch + 1}/{num_epochs}")

    # Training
    train_loss, train_accuracy = train_model(model, train_loader, criterion, optimizer, device)
    print(f"Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy * 100:.2f}%")

    # Evaluation
    print("Evaluating on Validation Set...")
    eval_accuracy = evaluate_model(model, val_loader, device)
    print(f"Validation Accuracy: {eval_accuracy * 100:.2f}%")

# Optionally, save the model after training
torch.save(model.state_dict(), "fasttext_ag_news_model.pth")
print("Model saved successfully!")